#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): NO
FEATURE - Dialogue for spell-checking: NO
FEATURE - Dialogue for disambiguation: NO
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: YES
FEATURE - Identifying and responding to emotions: YES
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
We both collaborated equally and did basically the entire project together in person or on the phone.


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
TODO: Please answer the following question:

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

Answer:
By overanthromorphizing chatbot systems, we as humans can come to easily much more easily trust their answers
and speech. Many people are much more likely to trust and accept the advice and answers of a chatbot that sounds more human
rather than one that sounds more robotic/nonanthropomorphized regardless of the actual content(even if they are given the 
same answers phrased slightly differently). This might lead to some possible suspicious and even dangerous ramifications;
for example, a human asking a chatbot for moral or ethical advice regarding a situation might overly trust a anthropo moporphizedcomputer that 
inherently does not have a moral code. The chatbot can easily be pushed into saying things that are racist, stereotyped, and harshly  discriminatory,
leading to further possible detriment to those who follow its word to closely. Chatbot systems may even be utilized in the future to purposefully
manipulate the gullible vulnerable into specific viewpoints or actions on a wide and effcicient scale.
Engineers could purposefully include warning notifications and such into chatbot responses in order to constantly and consistently remind
users that they are interacting with a machine and not a human. This could be even taken further when users are detected to have asked
potentially charged and more personal questions rather than matter of fact objective research questions; for example, if keywords such as "husband" or
"cousin", "kill", and even more show up, the engineer could make sure these warnings are first and foremost before any answer is returned.



#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!

